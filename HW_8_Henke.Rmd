## Problem 1

Week 1: In week 1 I learned more about the underfitting and overfitting of models as well as the types of error they can introduce into a model.  For the models, we can see both training and generalization errors.  For an ideal model, we want to have both of these errors close to each other, unless both are large, then in this case the model is underfit. If the generalization error is too large, the mdoel is overfit as it gives too much weight to the training data.

Week 2: In week 2 we took a look at ways to summarize our data with the calling of the summary() command, that gives information on various metrics for each variable in the dataset such as minimum, mean, median, and maximum.  In the discussion for week 2, we focused in on exploring the differences between explanatory analysis and exploratory analysis.  Exploratory analysis refers to analysis with the goal of investigating a data set that has not been defined clearly, whereas explanatory takes the methods used in exploratory analysis and seeks to explain the reasons for the relationships between the variables in the data set.

Week 3: Week 3 was focused around linear regression and some of its applications. I learned the structure of a linear model in that it models the expected value of a dependent variable based on various independent variables that can be categorical or numeric. For its applications, we looked at taking a data set, splitting it for test/train sets, making predictions from the created model, then assessed its quality using fitted vs residuals charts and normal q-q plots.

Week 4: Week 4 was instead focused on logistic regression and its application for its use in classification. When we have a model we want to develop yet we cannot quantify it in a continuous dependent variable, we use classification to solve the problem.  The qualitative response needed in these problems cannot be properly used in a normal linear regression model as the difference between the values assigned to each classification don't have any real meaning. Logistic regression comes in when we want to predict the probability that an instance of an experiment belongs to a category of the dependent variable.  To evaluate the logistic regression, we can use a confusion matrix to check both its precision and recall.

Week 5: Week 5 focused on the introduction of GLMs, which are much more flexible than a normal linear based model. It can account for a variety of distributions for the response variable.  These include standard normal, gamma, poisson, etc.  One characteristic of a GLM's flexibility is its ability to include offsets.  Offsets are predictors that can be set to be left out from having an influence on the response variable in an experiment.

Week 6: Decision trees and random forests were interesting to learn about in week 6. Decision trees are easy to understand and their use in real life applications for presenting relationships of data to management. Tree models handle categorical predictors very easily and can be used effectively in areas such as advertising.  Random forests combine several decision trees resulting in a slower overall process that requires more rigorous training. However, random forests prevent overfitting by using multiple trees.

Week 7: In week 7 I learned about K-Means clustering and its use to blindly distinguish hidden features in data sets through unsupervised learning.  Using the various plots to both determine the optimal number of clusters and the plot of the clusters was helpful as it can be a difficult concept to grasp.  For the discussion, I learned from an article I researched that unsupervised algorithims are very helpful for various forms of security and fraud prediction.

Week 8: Week 8 gave me some context into how we format out R markdown files for use in presentations, specifically creating ioslides. I also got some insight on what to prepare for future coworkers in presentations when presenting results, some items to include are the goals of the project, sources of data, and approaches taken to solve the problem.